{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq done all",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN7cQ3wOtCONWaz6dZh5mXY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shanu-gandhi-iiith/WikiSearchEngine/blob/master/seq_done_all.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gi1CA3ZlVTXO",
        "outputId": "31f9e534-54f4-479d-e0c7-ff01d8dd4ca8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "###############run once\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "%cd drive/My\\ Drive/wikiproj\n",
        "#login to smartcity.application.platformtorun"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/wikiproj\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-52TR4d8M7-",
        "outputId": "9fa56fc9-2971-47eb-eb5c-8a8c07ab7479",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "f1=open(\"2019201014_queries1.txt\",\"r\")\n",
        "linescur=f1.readlines()\n",
        "linescur2=[]\n",
        "for i in linescur:\n",
        "  linescur2.append(i.strip())\n",
        "print(linescur2)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['10, Kamienica Pomeranian Voivodeship', '10, t:gallery i:ware b:duchess', '20, b:1898 c:Leicester City Football League', '10, Interstate 74', '20, Mahendra Singh Dhoni', '10, t:Wroclaw Uniwersity i:Konigliche', '10, Missouri Route 85', '10, 25 December', '10, t:love b:Arrows', '5, t:Combat i:konami', '10, The Capture 1950', '5, b:sweden r:Pluxemburg', '20, t:Wave S8500 i:bada c:Mobile phones introduced in 2010', '20, b:William Owen c:English songwriters', '5, t:Fuaamotu i:Nukualofa', '20, t:de la Torre b:Badajoz', '10, t:russia tajikistan relation', '10, t:chinese democracy i:axl rose c:2008 albums', '3, b:V for Vendetta i:2006', \"5, young's modulus\", '10, b:Speed of light c:Concepts in physics', '20, b:angus r:tories i:James Lee', '30, b:Taxicab 1729', '50, b:Marc Spector i:Marvel Comics c:1980 comics debuts', '100, Sudebnik']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlCommUShYEq",
        "outputId": "28397bf5-3403-4b67-f83b-b045e4841030",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "############search \n",
        "######################################################\n",
        "#############          search part\n",
        "############BElIEVE IN GOD AND WORK HARD------------------ Everything else will fall in place\n",
        "############# created by shanu gandhi on 30 -08- 2020\n",
        "######   sec ind format\n",
        "# chunk_0:000\n",
        "# chunk_1:incab\n",
        "# chunk_2:variationsefncatherin\n",
        "# chunk_3:efnnamesurfac\n",
        "# chunk_4:postextorgwork550\n",
        "import pickle\n",
        "import nltk\n",
        "import re\n",
        "nltk.download('punkt',quiet=True)\n",
        "nltk.download('stopwords',quiet=True)\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer \n",
        "from nltk.stem import SnowballStemmer \n",
        "from nltk.tokenize import word_tokenize \n",
        "import math\n",
        "import time\n",
        "import sys\n",
        "import nltk\n",
        "from nltk.stem import SnowballStemmer \n",
        "from nltk.corpus import stopwords\n",
        "custset={\" \",\"a\", \"about\", \"above\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\", \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\",\"although\",\"always\",\"am\",\"among\", \"amongst\", \"amoungst\", \"amount\",  \"an\", \"and\", \"another\", \"any\",\"anyhow\",\"anyone\",\"anything\",\"anyway\", \"anywhere\", \"are\", \"around\", \"as\",  \"at\", \"back\",\"be\",\"became\", \"because\",\"become\",\"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\", \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\", \"bottom\",\"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\", \"could\",\"redirect\",\"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\", \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\",\"else\", \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\", \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\", \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\", \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"however\", \"hundred\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\", \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\", \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\", \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\",\"part\", \"per\", \"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thickv\", \"thin\", \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\", \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\", \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\", \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\", \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"wiktionary\", \"cite\",\"reflist\"}\n",
        "custset=custset.union(set(stopwords.words('english')))\n",
        "ps =nltk.stem.SnowballStemmer('english')\n",
        "\n",
        "startp=time.time()\n",
        "\n",
        "totalstr=\"\"\n",
        "# querystr = \"3, t:World Cup i:2019 c:Cricket\"\n",
        "\n",
        "########output in queries_op.txt\n",
        "\n",
        "##################\n",
        "# read index file\n",
        "count =-1\n",
        "# mapreaded={}\n",
        "start=time.time();\n",
        "\n",
        "reslis=[]\n",
        "map_docidattr={}\n",
        "pickle_in = open(\"indexsf/dict.pickle\",\"rb\")\n",
        "map_docidattr= pickle.load(pickle_in)\n",
        "\n",
        "\n",
        "# print(len(map_docidattr))\n",
        "# 't1-i3-c1-e3-b65-d38425':\n",
        "def parseandsort(k, curdfull):\n",
        "  global totalstr\n",
        "  curlis=curdfull.split(\"=\")\n",
        "  curdoc=curlis[1].strip()\n",
        "  # print(\"IN p&S\",curdoc)\n",
        "  ll=curdoc.split(\":\")\n",
        "  # print(str(ll))\n",
        "  ps=[]\n",
        "  for i in ll:\n",
        "    ilis=i.split(\"-\")  ### [t1 i3 c1 e3 b65 d38425]\n",
        "    cs=0\n",
        "    for j in ilis: ##j=b65\n",
        "      if j!=\"\" and j!=\" \" and len(j)> 1 and j[0]!='d':\n",
        "        # print(j,\":\",j[1:])\n",
        "        ##### Assign weight to title =5X , infobox=3X, Categ= 3X, ext link=1.5X, Refs=1.5X, Body=1X\n",
        "        if j[0]=='t':\n",
        "          cs+=((int(j[1:]))*5)\n",
        "        elif j[0]=='i':\n",
        "          cs+=((int(j[1:]))*3)\n",
        "        elif j[0]=='c':\n",
        "          cs+=((int(j[1:]))*3)\n",
        "        elif j[0]=='e':\n",
        "          cs+=((int(j[1:]))*1.5)\n",
        "        elif j[0]=='r':\n",
        "          cs+=((int(j[1:]))*1.5)\n",
        "        else:\n",
        "          cs+=(int(j[1:]))\n",
        "    docid=ilis[-1][1:]\n",
        "    if cs >0 :\n",
        "      ps.append([cs, docid]) \n",
        "  # print(str(ps))\n",
        "  ps.sort(key=lambda x: x[0],reverse =True)\n",
        "  # print(str(ps))\n",
        "  for i in range( min(int(k),len(ps)) ):\n",
        "    # print(\"11111\")\n",
        "    print((ps[i][1]),end =\", \")\n",
        "    print( map_docidattr[(ps[i][1])].split(\":\")[-9])\n",
        "    # for j in range(len(l11)-8):\n",
        "      # print( map_docidattr[(lisf[i][0])].split(\":\")[j],end=\" \")\n",
        "    totalstr+=str(ps[i][1])+\", \"+str(map_docidattr[(ps[i][1])].split(\":\")[0])+\"\\n\";\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def parseandsortm(topK,reslis):\n",
        "  global totalstr\n",
        "  lisg=[]\n",
        "  setdocs= set()\n",
        "  for i1 in reslis:\n",
        "    curdfull=i1\n",
        "    # print(str(i1))\n",
        "    curlis=curdfull.split(\"=\")\n",
        "    curdoc=curlis[1].strip()\n",
        "    # print(\"IN p&S\",curdoc)\n",
        "    ll=curdoc.split(\":\")\n",
        "    # print(str(ll))\n",
        "    plsize=len(ll)\n",
        "    # print(\"pls\",plsize)\n",
        "    # ps=[]\n",
        "    ps={}\n",
        "    for i in ll:\n",
        "\n",
        "      ilis=i.split(\"-\")  ### [t1 i3 c1 e3 b65 d38425]\n",
        "      cs=0\n",
        "      for j in ilis: ##j=b65\n",
        "        if j!=\"\" and j!=\" \" and len(j)> 1 and j[0]!='d':\n",
        "            cs+=(int(j[1:]))\n",
        "\n",
        "      docid=ilis[-1][1:]\n",
        "      if cs >0 :\n",
        "        if(len(map_docidattr[docid].split(\":\"))<2):\n",
        "          continue;\n",
        "        tw=map_docidattr[docid].split(\":\")[-2]  ####### m1[58576] =   Phosphor:2:1:6:2622:38:2:2671:\\n\n",
        "        if not tw.isnumeric():\n",
        "          continue;\n",
        "        tf=math.log(1+(int(cs)/int(tw)))\n",
        "        idf=math.log(len(map_docidattr)/int(plsize))\n",
        "        tfidf=round(tf*idf, 10)\n",
        "        ps[docid]=tfidf\n",
        "        setdocs.add(docid)\n",
        "    lisg.append(ps)\n",
        "  lisf=[]\n",
        "  for i in setdocs:\n",
        "    cur=0\n",
        "    ct=0\n",
        "    for j in lisg:\n",
        "      if i in j:\n",
        "        cur+=j[i]\n",
        "        ct=ct+1\n",
        "    lisf.append([i,ct,cur])\n",
        "  lisf = sorted(lisf, key = lambda x: (x[1], x[2]))\n",
        "  lisf.reverse()\n",
        "  for i in range( min(int(topK),len(lisf)) ):\n",
        "    print((lisf[i][0]), end =\", \")\n",
        "    # print(\"11112\")\n",
        "    print( map_docidattr[(lisf[i][0])].split(\":\")[-9])\n",
        "    totalstr+=str(lisf[i][0])+\", \"+str(map_docidattr[(lisf[i][0])].split(\":\")[0])+\"\\n\";\n",
        "\n",
        "\n",
        "def parseandsortfield(topK,reslis):\n",
        "  global totalstr\n",
        "  # print(topK)\n",
        "  lisg=[]\n",
        "  setdocs= set()\n",
        "  \n",
        "  for i1 in reslis:\n",
        "    # print(i)\n",
        "    curtag=i1[0:1]\n",
        "    curdfull=i1[2:]\n",
        "#    print(str(curdfull))\n",
        "    curlis=curdfull.split(\"=\")\n",
        "    curdoc=curlis[1].strip()\n",
        "    # print(\"IN p&S\",curdoc)\n",
        "    ll=curdoc.split(\":\")\n",
        "    # # print(str(ll))\n",
        "    plsize=len(ll)\n",
        "    # # print(\"pls\",plsize)\n",
        "    # ps=[]\n",
        "    ps={}\n",
        "    for i in ll:\n",
        "      ilis=i.split(\"-\")  ### [t1 i3 c1 e3 b65 d38425]\n",
        "      cs=0\n",
        "      for j in ilis: ##j=b65 ####adding all value here add only tag value req\n",
        "        if j!=\"\" and j!=\" \" and len(j)> 1 and j[0]!='d'and j[0]==curtag:\n",
        "            cs+=(int(j[1:]))\n",
        "\n",
        "      docid=ilis[-1][1:]\n",
        "      if cs >0 :\n",
        "        # print(map_docidattr[docid])\n",
        "        l1=map_docidattr[docid].split(\":\")\n",
        "        if len(l1)>7:\n",
        "          tw=l1[-2]  ####### m1[58576] =   Phosphor:2:1:6:2622:38:2:2671:\\n\n",
        "          if not tw.isnumeric():\n",
        "            continue;\n",
        "          tf=math.log(1+(int(cs)/int(tw)))\n",
        "          idf=math.log(len(map_docidattr)/int(plsize))\n",
        "          tfidf=round(tf*idf, 10)\n",
        "          ps[docid]=tfidf\n",
        "          setdocs.add(docid)\n",
        "    lisg.append(ps)\n",
        "  # for i in lisg:\n",
        "  #   print(i)\n",
        "  lisf=[]\n",
        "  for i in setdocs:\n",
        "    cur=0\n",
        "    ct=0\n",
        "    for j in lisg:\n",
        "      if i in j:\n",
        "        cur+=j[i]\n",
        "        ct=ct+1\n",
        "    lisf.append([i,ct,cur])\n",
        "  lisf = sorted(lisf, key = lambda x: (x[1], x[2]))\n",
        "  lisf.reverse()\n",
        "  for i in range( min(int(topK),len(lisf)) ):\n",
        "    print( (lisf[i][0]) ,end =\", \")\n",
        "    # print(\"333333\")\n",
        "    print( map_docidattr[(lisf[i][0])].split(\":\")[-9])\n",
        "    totalstr+=str(lisf[i][0])+\", \"+str(map_docidattr[(lisf[i][0])].split(\":\")[0])+\"\\n\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def searchnormal(query):\n",
        "  global reslis\n",
        "  stmq=ps.stem(query)\n",
        "  # print(\"normal:\",stmq)\n",
        "  # sind=open(\"indexs/chunksf/secndind\",\"r\")\n",
        "  sind=open(\"indexsf/chunksdir/secndind\",\"r\")\n",
        "  sline =sind.readlines()\n",
        "  ftosearch=\"\"\n",
        "  for i in range(len(sline)-1):\n",
        "    curfw=sline[i].split(\":\")[1].strip()\n",
        "    nextfw=sline[i+1].split(\":\")[1].strip()\n",
        "    if curfw <= stmq and nextfw > stmq:\n",
        "      ftosearch=sline[i].split(\":\")[0].strip()\n",
        "      # print(\"found\")\n",
        "      break\n",
        "  if ftosearch==\"\":\n",
        "    ftosearch=sline[-1].split(\":\")[0].strip()\n",
        "  # print(\"chunk to searc\", ftosearch)\n",
        "  fc=open(\"indexsf/chunksdir/\"+ftosearch,\"r\")\n",
        "  while True:\n",
        "    line=fc.readline()\n",
        "    if not line or line[0]>stmq[0]:\n",
        "      break\n",
        "    if line.startswith(stmq+\"=\"):\n",
        "      # print(line)\n",
        "      reslis.append(line)\n",
        "      break\n",
        "  # print(\"\")\n",
        "\n",
        "def searchcateg(query):\n",
        "  global reslis\n",
        "  tag=query[0:1]\n",
        "  query=query[2:]\n",
        "  lisq=query.split()\n",
        "  lisq.insert(0,tag)\n",
        "  # print(lisq)\n",
        "  curt=\"\"\n",
        "  sind=open(\"indexsf/chunksdir/secndind\",\"r\")\n",
        "  sline =sind.readlines()\n",
        "  for j in lisq:\n",
        "    if j=='i' or j=='t'or j=='c'or j=='r'or j=='b'or j=='e':\n",
        "      curt=j\n",
        "      continue;\n",
        "    stmq=ps.stem(j)\n",
        "    # print(curt, \":\",stmq)\n",
        "    ftosearch=\"\"\n",
        "\n",
        "    for i in range(len(sline)-1):\n",
        "      curfw=sline[i].split(\":\")[1].strip()\n",
        "      nextfw=sline[i+1].split(\":\")[1].strip()\n",
        "      if curfw <= stmq and nextfw > stmq:\n",
        "        ftosearch=sline[i].split(\":\")[0].strip()\n",
        "        # print(\"found\")\n",
        "        break\n",
        "    \n",
        "    if ftosearch==\"\":\n",
        "      # print(\"line\",sline)\n",
        "      ftosearch=sline[-1].split(\":\")[0].strip()\n",
        "    # print(\"chunk to searc\", ftosearch)\n",
        "    fc=open(\"indexsf/chunksdir/\"+ftosearch,\"r\")\n",
        "    while True:\n",
        "      line=fc.readline()\n",
        "      if not line or line[0]>stmq[0]:\n",
        "        break\n",
        "      if line.startswith(stmq+\"=\"):\n",
        "        # print(line)\n",
        "        reslis.append(curt+\":\"+line)\n",
        "        break\n",
        "    # print(\"\")\n",
        "  return \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#####################   main execution\n",
        "\n",
        "# readdocidtitle()\n",
        "# querystr = \"10, Narendra\"\n",
        "# querystr = \"10, Shanu\"\n",
        "\n",
        "# fq=open(\"queries.txt\",\"r\")\n",
        "# listqr=fq.readlines()\n",
        "listqr=[]\n",
        "# listqr.append(\"3, t:World Cup i:2019 c:Cricket\")\n",
        "# listqr.append(\"3, t:Hindi \")\n",
        "# listqr.append(\"3, Cup\")\n",
        "# listqr.append(\"2, t:the two towers i:1954\")\n",
        "# listqr.append(\"3, sachin\")\n",
        "# listqr.append(\"7, iiit\")\n",
        "# listqr.append(\"4, Bollywood\")\n",
        "# listqr.append(\"4, Burhanpur\")\n",
        "# listqr.append(\"4, Hyderabad\")\n",
        "# listqr.append(\"4, Mumbai\")\n",
        "# listqr.append(\"4, t:Tanhaji\")\n",
        "listqr.append(\"3, b:V for Vendetta i:2006\")\n",
        "listqr.append(\"5, young's modulus\")\n",
        "\n",
        "# listqr=linescur2\n",
        "\n",
        "for i11 in listqr:\n",
        "  # querystr=\"3, t:World Cup i:2019 c:Cricket\"\n",
        "  querystr=i11\n",
        "  # print(\"\\n\\nquerystr is\",querystr)\n",
        "  q=querystr\n",
        "  q=q.lower()\n",
        "  qlis=q.split(\",\")\n",
        "  topK=qlis[0]\n",
        "  # print(topK)\n",
        "  q=qlis[1]\n",
        "  if q[2]!=\":\":\n",
        "    st=time.time()\n",
        "    # print(\"Norm str\")\n",
        "    reslis=[]\n",
        "    lis=q.split(' ')\n",
        "    for i in lis:\n",
        "      if i!=\"\":\n",
        "        searchnormal(i)\n",
        "    # print(reslis)\n",
        "    if len(reslis)==0:\n",
        "      print(\"No Document Found\")\n",
        "      totalstr+=\"No Document Found\\n\"\n",
        "      pass\n",
        "    elif len(reslis)==1:\n",
        "      parseandsort(topK,reslis[0])\n",
        "    else:\n",
        "      # print(\"MULTIPLE WORDS\")\n",
        "      parseandsortm(topK,reslis)\n",
        "    print(round(time.time()-st ,2),\", \",round((time.time()-st)/int(topK) ,2))\n",
        "    totalstr+=str(round(time.time()-st ,2))+\", \"+str(round((time.time()-st)/int(topK) ,2))+\"\\n\"\n",
        "  else:\n",
        "    # print(\"Categ query\")\n",
        "    st=time.time()\n",
        "    j=0\n",
        "    cs=\"\"\n",
        "    q=q[1:]\n",
        "    # print(q)\n",
        "    lisq=q.split(\":\")\n",
        "    # print(lisq)\n",
        "    flis=[]\n",
        "    cur=\"\"\n",
        "    flis.append(lisq[0]+\":\"+lisq[1][:-2])\n",
        "    for i in range(len(lisq)-1):\n",
        "      if i==0:\n",
        "        continue\n",
        "      if i==len(lisq)-2:\n",
        "        cur=cur+lisq[i][-1]+\":\"+lisq[i+1]\n",
        "      else:\n",
        "        cur=cur+lisq[i][-1]+\":\"+lisq[i+1][:-1]\n",
        "      cur=cur.strip()\n",
        "      flis.append(cur)\n",
        "      cur=\"\"\n",
        "    # print(flis)\n",
        "    reslis=[]\n",
        "    for i in flis:\n",
        "      searchcateg(i)\n",
        "    # print(len(reslis))\n",
        "    # for i in reslis:\n",
        "    #   print(i)\n",
        "    if len(reslis)==0:\n",
        "      print(\"No Document Found\")\n",
        "      totalstr+\"No Document Found\\n\"\n",
        "      pass\n",
        "    else:\n",
        "      # print(\"MULTIPLE DOCLIST\")\n",
        "      parseandsortfield(topK,reslis)\n",
        "    print(round(time.time()-st ,2),\", \",round((time.time()-st)/int(topK) ,2))\n",
        "    totalstr+=str(round(time.time()-st ,2))+\", \"+str(round((time.time()-st)/int(topK) ,2));\n",
        "  print(\"\")\n",
        "  totalstr+=\"\\n\\n\"\n",
        "\n",
        "\n",
        "fileo=open(\"queries_op.txt\", \"w\")\n",
        "fileo.write(totalstr)\n",
        "fileo.close()\n",
        "print(time.time() -startp)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6467441,  Cuca\n",
            "7706433, Vendetta (Celesty album)\n",
            "3177689, Laura Greenwood\n",
            "3.89 ,  1.3\n",
            "\n",
            "863489, Young Modulus\n",
            "3743279, Youngs Modulus\n",
            "9758048, Youngs' Modulus\n",
            "5827660, Youngâ€™s modulus\n",
            "3743281, Youngs modulus\n",
            "2.6 ,  0.52\n",
            "\n",
            "14.220653533935547\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ck447KBkVIyP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTuuQYVzZDVr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wxv_nEPVfbG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cm5Bf4VAjZ5U"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Jl3tzIBtdil"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-fx1GVSndgF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAOa0IRCnexI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrbEkWIHng2h"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwU1skoWZLcG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osomVXquZanT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ga7Ba_oOE67d"
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWbUeGXAFo6C"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3Z5bJi9Ftdp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpfj6XtDFwep",
        "outputId": "4f2ba1f9-50dc-4bfa-8f29-060fac8d1d29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(map_docidattr))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9829059\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uYL6uBjAjPD",
        "outputId": "104d2af5-4cf1-4171-9a50-32621a5d88fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "j=0\n",
        "for i in map_docidattr:\n",
        "  print(i, map_docidattr[i])\n",
        "  j=j+1\n",
        "  if j==100:\n",
        "    break"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 AccessibleComputing:2:1:1:5:1:1:11:\n",
            "2 Anarchism:2:1:12:3566:99:1:3681:\n",
            "3 AfghanistanHistory:2:1:1:6:1:1:12:\n",
            "4 AfghanistanGeography:2:1:1:6:1:1:12:\n",
            "5 AfghanistanPeople:2:1:1:6:1:1:12:\n",
            "6 AfghanistanCommunications:2:1:1:6:1:1:12:\n",
            "7 AfghanistanTransportations:2:1:1:7:1:1:13:\n",
            "8 AfghanistanMilitary:2:1:1:7:1:1:13:\n",
            "9 AfghanistanTransnationalIssues:2:1:1:7:1:1:13:\n",
            "10 AssistiveTechnology:2:1:1:5:1:1:11:\n",
            "11 AmoeboidTaxa:2:1:1:5:1:1:11:\n",
            "12 Autism:2:58:11:4830:47:1:4949:\n",
            "13 AlbaniaHistory:2:1:1:6:1:1:12:\n",
            "14 AlbaniaPeople:2:1:1:6:1:1:12:\n",
            "15 AsWeMayThink:2:1:1:5:1:1:11:\n",
            "16 AlbaniaGovernment:2:1:1:6:1:1:12:\n",
            "17 AlbaniaEconomy:2:1:1:6:1:1:12:\n",
            "18 Albedo:2:1:10:1950:24:67:2054:\n",
            "19 AfroAsiaticLanguages:2:1:1:6:1:1:12:\n",
            "20 ArtificalLanguages:2:1:1:9:1:1:15:\n",
            "21 AbacuS:2:1:1:5:1:1:11:\n",
            "22 AbalonE:2:1:1:5:1:1:11:\n",
            "23 AbbadideS:2:1:1:6:1:1:12:\n",
            "24 AbbesS:2:1:1:5:1:1:11:\n",
            "25 AbbevilleFrance:2:1:1:5:1:1:11:\n",
            "26 AbbeY:2:1:1:5:1:1:11:\n",
            "27 AbboT:2:1:1:5:1:1:11:\n",
            "28 Abbreviations:2:1:1:5:1:1:11:\n",
            "29 AtlasShrugged:2:1:1:6:1:1:12:\n",
            "30 ArtificialLanguages:2:1:1:6:1:1:12:\n",
            "31 AtlasShruggedCharacters:2:1:1:8:1:1:14:\n",
            "32 AtlasShruggedCompanies:2:1:1:6:1:1:12:\n",
            "33 AyersMusicPublishingCompany:2:1:1:7:1:1:13:\n",
            "34 AfricanAmericanPeople:2:1:1:6:1:1:12:\n",
            "35 AdolfHitler:2:1:1:6:1:1:12:\n",
            "36 AbeceDarians:2:1:1:5:1:1:11:\n",
            "37 AbeL:2:1:1:6:1:1:12:\n",
            "38 AbensbergGermany:2:1:1:5:1:1:11:\n",
            "39 AberdeenSouthDakota:2:1:1:7:1:1:13:\n",
            "40 ArthurKoestler:2:1:1:6:1:1:12:\n",
            "41 AynRand:2:1:1:6:1:1:12:\n",
            "42 AlexanderTheGreat:2:1:1:6:1:1:12:\n",
            "43 AnchorageAlaska:2:1:1:6:1:1:12:\n",
            "44 ArgumentForms:2:1:1:6:1:1:12:\n",
            "45 ArgumentsForTheExistenceOfGod:2:1:1:6:1:1:12:\n",
            "46 AnarchY:2:1:1:5:1:1:11:\n",
            "47 AsciiArt:2:1:1:6:1:1:12:\n",
            "48 AcademyAwards:2:1:1:6:1:1:12:\n",
            "49 AcademyAwards/BestPicture:2:1:1:9:1:1:15:\n",
            "50 AustriaLanguage:2:1:1:6:1:1:12:\n",
            "51 AcademicElitism:2:1:1:6:1:1:12:\n",
            "52 AxiomOfChoice:2:1:1:6:1:1:12:\n",
            "53 AmericanFootball:2:1:1:6:1:1:12:\n",
            "54 AmericA:2:1:1:4:1:1:10:\n",
            "55 AnnaKournikova:2:1:1:4:1:1:10:\n",
            "56 AndorrA:2:1:1:5:1:1:11:\n",
            "57 AustroAsiaticLanguages:2:1:1:6:1:1:12:\n",
            "58 ActresseS:2:1:1:7:1:1:13:\n",
            "59 A:1:52:3:1102:21:1:1180:\n",
            "60 AnarchoCapitalism:2:1:1:5:1:1:11:\n",
            "61 AnarchoCapitalists:2:1:1:5:1:1:11:\n",
            "62 ActressesS:2:1:1:6:1:1:12:\n",
            "63 AnAmericanInParis:2:1:1:6:1:1:12:\n",
            "64 AutoMorphism:2:1:1:5:1:1:11:\n",
            "65 ActionFilm:2:1:1:6:1:1:12:\n",
            "66 Alabama:2:9250:9:17:1:1:9280:\n",
            "67 AfricA:2:1:1:5:1:1:11:\n",
            "68 Achilles:2:1:9:5254:22:1:5289:\n",
            "69 AppliedStatistics:2:1:1:5:1:1:11:\n",
            "70 Abraham Lincoln:3:9979:50:21:1:1:10055:\n",
            "71 Aristotle:2:7091:89:23:1:1:7207:\n",
            "72 An American in Paris:3:1:7:1205:53:1:1270:\n",
            "73 Academy Award for Best Production Design:6:35:4:116:1:13:175:\n",
            "74 Academy Awards:3:1183:13:4492:36:1:5728:\n",
            "75 Action Film:3:1:1:6:1:1:13:\n",
            "76 Actrius:2:351:8:5:1:1:368:\n",
            "77 Animalia (book):3:47:6:225:22:1:304:\n",
            "78 International Atomic Time:4:1:2:718:55:3:783:\n",
            "79 Altruism:2:1:11:3810:29:63:3916:\n",
            "80 AutoRacing:2:1:1:6:1:1:12:\n",
            "81 Ayn Rand:3:4291:82:11:1:1:4389:\n",
            "82 Alain Connes:3:376:21:10:1:1:412:\n",
            "83 Allan Dwan:3:1013:14:11:1:1:1043:\n",
            "84 Algeria/People:2:1:1:6:1:1:12:\n",
            "85 Algeria/Transnational Issues:3:1:1:7:1:1:14:\n",
            "86 Algeria:2:7679:23:16:1:1:7722:\n",
            "87 List of Atlas Shrugged characters:5:1:5:2500:14:1:2526:\n",
            "88 Topics of note in Atlas Shrugged:5:1:1:3:1:1:12:\n",
            "89 Anthropology:2:1:2:5039:13:1:5058:\n",
            "90 Agricultural science:3:1:4:888:70:1:967:\n",
            "91 Alchemy:2:1:6:6284:6:1:6300:\n",
            "92 Alien:2:1:1:547:1:1:553:\n",
            "93 Astronomer:2:1:4:457:22:1:487:\n",
            "94 Ameboid stage:3:1:1:2:1:1:9:\n",
            "95 ASCII:2:74:8:3357:3:11:3455:\n",
            "96 Ashmore And Cartier Islands:4:1:1:7:1:1:15:\n",
            "97 Austin (disambiguation):3:1:1:197:1:1:204:\n",
            "98 Animation:2:1:5:3767:38:1:3814:\n",
            "99 Apollo:2:162:19:13232:24:3:13442:\n",
            "100 Andre Agassi:3:506:41:5102:120:2:5774:\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qN8b4aRnB0tg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
